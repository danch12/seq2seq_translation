{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Attention as AttentionLayer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from unicodedata import normalize\n",
    "import string\n",
    "import keras\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import warnings\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input, Bidirectional, LSTM, Dense, Concatenate\n",
    "from tensorflow.keras.initializers import Constant\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = \"drive/My Drive/\"\n",
    "path = default_path + 'cnn/fr-en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T09:40:12.463459Z",
     "start_time": "2020-07-21T09:40:12.439639Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    result = []\n",
    "    # regex for removing weird chars\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    regex_punct = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    for line in lines:\n",
    "        # unicode chars\n",
    "        line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "        line = line.decode('UTF-8')\n",
    "        # split on whitespace so we can remove weird chars and punctuation\n",
    "        line = line.split()\n",
    "        # convert to lower case\n",
    "        line = [word.lower() for word in line]\n",
    "        # remove punctuation\n",
    "        line = [regex_punct.sub('', word) for word in line]\n",
    "        # remove weird chars\n",
    "        line = [re_print.sub('', w) for w in line]\n",
    "        #remove numbers\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "        result.append(' '.join(line))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load english text\n",
    "with open(path + '/' + 'europarl-v7.fr-en.fr', encoding='utf-8') as f:\n",
    "    english_text = f.read()\n",
    "\n",
    "#load french data\n",
    "with open(path + '/' + 'europarl-v7.fr-en.en', encoding='utf-8') as f:\n",
    "    french_text = f.read()\n",
    "\n",
    "clean_eng = clean_data(english_text)\n",
    "clean_french = clean_data(french_text)\n",
    "#save the files so we dont have to do this process again\n",
    "with open('clean_english.pkl', 'wb') as f:\n",
    "    pickle.dump(clean_eng, f)\n",
    "\n",
    "with open('clean_french.pkl', 'wb') as f:\n",
    "    pickle.dump(clean_french, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_path = path + 'clean_french.pkl'\n",
    "english_path = path + 'clean_english.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 40\n",
    "#cut down length of sentences to speed up training times\n",
    "shortened_english = [word_tokenize(x)[:max_length] for x in english_data]\n",
    "shortened_french = [['<s>'] + word_tokenize(x)[:max_length - 2] + ['</s>']\n",
    "                    for x in french_data]\n",
    "#add padding to english sentences to make them all same length\n",
    "shortened_english = [\n",
    "    d + (max_length - len(d)) * [\"<padding>\"] for d in shortened_english\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating list of words in english\n",
    "english_words = list()\n",
    "for sentence in shortened_english:\n",
    "    for word in sentence:\n",
    "        if word != \"<padding>\":\n",
    "            english_words.append(word)\n",
    "\n",
    "#putting the most common words into a dict\n",
    "english_word_counter = collections.Counter(english_words).most_common()[:10000]\n",
    "english_word_dict = dict()\n",
    "english_word_dict[\"<padding>\"] = 0\n",
    "english_word_dict[\"<unk>\"] = 1\n",
    "english_word_dict[\"<s>\"] = 2\n",
    "english_word_dict[\"</s>\"] = 3\n",
    "for word, _ in english_word_counter:\n",
    "    english_word_dict[word] = len(english_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating list of french words\n",
    "french_words = list()\n",
    "for sentence in shortened_french:\n",
    "    for word in sentence:\n",
    "        if word != \"<padding>\" and word != \"<s>\" and word != \"</s>\":\n",
    "            french_words.append(word)\n",
    "\n",
    "#putting the most common words into a dict\n",
    "french_word_counter = collections.Counter(french_words).most_common()[:10000]\n",
    "french_word_dict = dict()\n",
    "french_word_dict[\"<padding>\"] = 0\n",
    "french_word_dict[\"<unk>\"] = 1\n",
    "french_word_dict[\"<s>\"] = 2\n",
    "french_word_dict[\"</s>\"] = 3\n",
    "for word, _ in french_word_counter:\n",
    "    french_word_dict[word] = len(french_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining reversed dicts for inference\n",
    "english_reversed_dict = dict(zip(english_word_dict.values(), english_word_dict.keys()))\n",
    "french_reversed_dict = dict(zip(french_word_dict.values(), french_word_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialising the training data\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(shortened_english), max_length),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(shortened_french), max_length),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(shortened_french), max_length),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text,target_text) in enumerate(zip(shortened_english, shortened_french)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        try:\n",
    "            #use unk if we cant find the word in the dictionary\n",
    "            encoder_input_data[i, t] = english_word_dict.get(\n",
    "                char, english_word_dict[\"<unk>\"])\n",
    "        except:\n",
    "            print(char)\n",
    "\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = french_word_dict.get(\n",
    "            char, french_word_dict[\"<unk>\"])\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1] = french_word_dict.get(\n",
    "                char, french_word_dict[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data to get validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "encoder_input_data_train, encoder_input_data_test, decoder_input_data_train, decoder_input_data_test, decoder_target_data_train, decoder_target_data_test = train_test_split(\n",
    "    encoder_input_data, decoder_input_data, decoder_target_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"glove/model_glove_300.pkl\", 'rb') as handle:\n",
    "        word_vectors = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "embedding_matrix = np.zeros((len(english_word_dict) + 1, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in english_word_dict.items():\n",
    "      # if the word can be found in the glove word vector then add the weight\n",
    "      # if not then just keep it blank\n",
    "    try:\n",
    "        embedding_vector = word_vectors.word_vec(word)\n",
    "    except:\n",
    "        embedding_vector=None\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        path+'models/{epoch:02d}-{val_loss:.2f}.h5',\n",
    "        verbose=1,\n",
    "        save_best_only=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "latent_dim = 300\n",
    "num_words = 10004\n",
    "units = 128\n",
    "#put the glove embeddings in here\n",
    "embedding_layer = Embedding(num_words + 1,\n",
    "                            latent_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)\n",
    "\n",
    "#encoder model\n",
    "encoder_inputs = Input(shape=(max_length, ), name=\"encoder_input\")\n",
    "encoder_emb = embedding_layer(encoder_inputs)\n",
    "encoder_lstm_1 = LSTM(latent_dim, return_state=True, return_sequences=True)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm_1(encoder_emb)\n",
    "encoder_lstm_2 = LSTM(latent_dim, return_state=True, return_sequences=True)\n",
    "\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm_2(encoder_output1)\n",
    "\n",
    "#decoder model\n",
    "decoder_inputs = Input(shape=(None, ))\n",
    "decoder_emb_layer = Embedding(num_words, latent_dim, trainable=True)\n",
    "decoder_emb = decoder_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_emb,\n",
    "                                     initial_state=[state_h2, state_c2])\n",
    "#attention layer\n",
    "attention = AttentionLayer()\n",
    "attn_out, attn_states = attention([encoder_output2, decoder_outputs])\n",
    "#combine attention with decoder outputs\n",
    "decoder_outputs = Concatenate(axis=-1)([decoder_outputs, attn_out])\n",
    "\n",
    "decoder_dense = TimeDistributed(\n",
    "    Dense(num_words, activation='softmax', name=\"Dense_layer\"))\n",
    "\n",
    "decoder_final_outputs = decoder_dense(decoder_outputs)\n",
    "seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_final_outputs)\n",
    "#use sparse categorical as our data is not one hot encoded\n",
    "seq2seq_Model.compile(optimizer='rmsprop',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training model\n",
    "history = seq2seq_Model.fit(\n",
    "    [encoder_input_data_train, decoder_input_data_train],\n",
    "    decoder_target_data_train,\n",
    "    epochs=10,\n",
    "    validation_data=([encoder_input_data_test,\n",
    "                      decoder_input_data_test], decoder_target_data_test),\n",
    "    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder inference model\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_output2,state_h2, state_c2])\n",
    "\n",
    "#decoder inference model\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(40,latent_dim))\n",
    "dec_emb2=decoder_emb_layer(decoder_inputs)\n",
    "decoder_outputs_inf, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "attn_out_inf,attn_out_states=attention([decoder_hidden_state_input, decoder_outputs_inf])\n",
    "\n",
    "decoder_inf_concat=tf.keras.layers.Concatenate()([decoder_outputs_inf, attn_out_inf])\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "decoder_model = Model(\n",
    "[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "[decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding incoming data\n",
    "e_out, e_h, e_c = encoder_model.predict(test_data.reshape(1, -1))\n",
    "#creating target sequence\n",
    "target_seq = np.zeros((1, 1))\n",
    "#inputing start token\n",
    "target_seq[0, 0] = english_word_dict['<s>']\n",
    "stop_condition = False\n",
    "decoded_sentence = ''\n",
    "#looping until stop token is predicted or\n",
    "while not stop_condition:\n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] +\n",
    "                                                [e_out, e_h, e_c])\n",
    "    #want to get index with highest probability\n",
    "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    #find word at that index\n",
    "    sampled_char = french_reversed_dict[sampled_token_index]\n",
    "    #add to sentence\n",
    "    decoded_sentence += ' ' + sampled_char\n",
    "    if (sampled_char == \"</s>\" or len(word_tokenize(decoded_sentence)) > 52):\n",
    "        stop_condition = True\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = sampled_token_index\n",
    "    #update hidden + cell states\n",
    "    e_h, e_c = h, c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
